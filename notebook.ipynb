{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marimo as mo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hbol",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vblA",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "from collections import Counter, defaultdict\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.layouts import column\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lEQa",
   "metadata": {
    "marimo": {
     "config": {
      "hide_code": true
     }
    }
   },
   "source": [
    "# CS4287-CNN: Construction Safety Equipment Detection\n",
    "\n",
    "**Authors**: MYKOLA VASKEVYCH (22372199), Teammate Name (ID2)\n",
    "\n",
    "**Status**: Code executes to completion: YES\n",
    "\n",
    "## Overview\n",
    "This notebook fine-tunes a YOLOv8 nano model to detect Personal Protective Equipment (PPE)\n",
    "violations on construction sites. The model identifies safety equipment (hardhats, masks,\n",
    "safety vests) and flags violations when workers lack proper protection.\n",
    "\n",
    "## Dataset\n",
    "- **Classes**: 10 (Hardhat, Mask, NO-Hardhat, NO-Mask, NO-Safety Vest, Person, Safety Cone, Safety Vest, machinery, vehicle)\n",
    "- **Format**: YOLO format with normalized bounding boxes\n",
    "- **Splits**: Train/Validation/Test\n",
    "\n",
    "## Quick Start\n",
    "1. Ensure dataset is in `data/archive/css-data/` directory\n",
    "2. Run all cells sequentially (training will not start automatically)\n",
    "3. Review dataset statistics and quality\n",
    "4. Click \"Train Model\" button when ready to train\n",
    "5. Scroll down to see training results and model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PKri",
   "metadata": {},
   "source": [
    "# CHECKS & SETTINGS\n",
    "## NOTE: CHECK CONSTANTS BELOW TO ENSURE CORRECTNESS BEFORE RUNNING THE NOTEBOOK.\n",
    "\n",
    "**⚡ Quick Navigation**: [Jump to Train Button](#training-configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_yaml = mo.ui.checkbox(\n",
    "    label=\"# Regenerate YAML file (uncheck to skip if file is correct)\",\n",
    "    value=False\n",
    ")\n",
    "regenerate_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFPL",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Auto-detect best available device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 0\n",
    "    _device_name = torch.cuda.get_device_name(0)\n",
    "    _vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU detected: {_device_name} ({_vram_gb:.1f}GB VRAM)\")\n",
    "    print(f\"Recommended BATCH_SIZE: {16 if _vram_gb >= 8 else 8}\")\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    print(\"⚠ No GPU detected - training will be significantly slower\")\n",
    "    print(\"Recommended: Reduce EPOCHS to 10 and BATCH_SIZE to 4 for CPU\")\n",
    "\n",
    "# Dataset paths\n",
    "DATASET_ROOT = Path.cwd() / \"data\" / \"archive\" / \"css-data\"\n",
    "TRAINING_IMAGES_PATH = (DATASET_ROOT / \"train\" / \"images\").resolve()\n",
    "TRAINING_LABELS_PATH = (DATASET_ROOT / \"train\" / \"labels\").resolve()\n",
    "VALIDATION_IMAGES_PATH = (DATASET_ROOT / \"valid\" / \"images\").resolve()\n",
    "VALIDATION_LABELS_PATH = (DATASET_ROOT / \"valid\" / \"labels\").resolve()\n",
    "TEST_IMAGES_PATH = (DATASET_ROOT / \"test\" / \"images\").resolve()\n",
    "TEST_LABELS_PATH = (DATASET_ROOT / \"test\" / \"labels\").resolve()\n",
    "YAML_CONFIG_PATH = DATASET_ROOT / \"data.yaml\"\n",
    "\n",
    "# Model paths\n",
    "PRETRAINED_MODEL_PATH = \"yolov8n.pt\"\n",
    "TRAINING_OUTPUT_DIR = \"runs/train\"\n",
    "TRAINING_RUN_NAME = \"ppe_detection\"\n",
    "TRAINED_MODEL_PATH = (\n",
    "    Path(TRAINING_OUTPUT_DIR) / TRAINING_RUN_NAME / \"weights\" / \"best.pt\"\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10  # Reduce to 10 for quick testing or CPU training\n",
    "IMAGE_SIZE = 640  # YOLO standard input size\n",
    "BATCH_SIZE = 16  # Reduce to 4-8 for low VRAM or CPU\n",
    "\n",
    "# Detection parameters\n",
    "CONFIDENCE_THRESHOLD = 0.25  # Minimum confidence for detections (0.0-1.0)\n",
    "\n",
    "# Visualization parameters\n",
    "NUM_SAMPLE_IMAGES = 6\n",
    "NUM_COMPARISON_IMAGES = 4\n",
    "NUM_BASELINE_TEST_SAMPLES = 3\n",
    "\n",
    "# Class definitions\n",
    "CLASS_NAMES = {\n",
    "    0: \"Hardhat\",\n",
    "    1: \"Mask\",\n",
    "    2: \"NO-Hardhat\",\n",
    "    3: \"NO-Mask\",\n",
    "    4: \"NO-Safety Vest\",\n",
    "    5: \"Person\",\n",
    "    6: \"Safety Cone\",\n",
    "    7: \"Safety Vest\",\n",
    "    8: \"machinery\",\n",
    "    9: \"vehicle\",\n",
    "}\n",
    "\n",
    "# Bounding box colors (BGR format for OpenCV)\n",
    "BBOX_COLORS = {\n",
    "    0: (0, 255, 0),      # Hardhat - Green\n",
    "    1: (255, 255, 0),    # Mask - Cyan\n",
    "    2: (0, 0, 255),      # NO-Hardhat - Red\n",
    "    3: (0, 0, 255),      # NO-Mask - Red\n",
    "    4: (0, 0, 255),      # NO-Safety Vest - Red\n",
    "    5: (255, 0, 255),    # Person - Magenta\n",
    "    6: (0, 165, 255),    # Safety Cone - Orange\n",
    "    7: (0, 255, 0),      # Safety Vest - Green\n",
    "    8: (128, 128, 128),  # machinery - Gray\n",
    "    9: (255, 0, 0),      # vehicle - Blue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BYtC",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate dataset structure exists\n",
    "_missing_paths = []\n",
    "_paths_to_check = {\n",
    "    \"Dataset root\": DATASET_ROOT,\n",
    "    \"Training images\": TRAINING_IMAGES_PATH,\n",
    "    \"Validation images\": VALIDATION_IMAGES_PATH,\n",
    "    \"Test images\": TEST_IMAGES_PATH,\n",
    "}\n",
    "\n",
    "for _name, _path in _paths_to_check.items():\n",
    "    if not _path.exists():\n",
    "        _missing_paths.append(f\"- {_name}: `{_path}`\")\n",
    "\n",
    "if _missing_paths:\n",
    "    _error_msg = \"**ERROR: Missing required paths:**\\n\\n\" + \"\\n\".join(_missing_paths)\n",
    "    _error_msg += \"\\n\\n**Please ensure dataset is extracted to the correct location.**\"\n",
    "    mo.stop(True, mo.md(_error_msg))\n",
    "\n",
    "print(\"✓ All dataset paths validated successfully\\n\")\n",
    "\n",
    "# Display configuration summary\n",
    "mo.md(\n",
    "    f\"\"\"\n",
    "    ## Current Configuration\n",
    "\n",
    "    | Parameter | Value | Description |\n",
    "    |-----------|-------|-------------|\n",
    "    | Device | `{DEVICE}` | Training device (0=GPU, 'cpu'=CPU) |\n",
    "    | Epochs | `{EPOCHS}` | Training iterations through dataset |\n",
    "    | Image Size | `{IMAGE_SIZE}px` | Input resolution |\n",
    "    | Batch Size | `{BATCH_SIZE}` | Images per training step |\n",
    "    | Confidence | `{CONFIDENCE_THRESHOLD}` | Min score for detections |\n",
    "\n",
    "    **Note**: Adjust BATCH_SIZE in constants cell if you get OOM (Out of Memory) errors.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RGSE",
   "metadata": {},
   "source": [
    "## Dataset Configuration\n",
    "\n",
    "Generate YOLO-compatible data.yaml configuration file and verify dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Kclp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate YAML using CLASS_NAMES constant to ensure consistency\n",
    "_names_yaml = \"\\n  \".join([f\"{k}: {v}\" for k, v in CLASS_NAMES.items()])\n",
    "\n",
    "_yaml_content = f\"\"\"path: {DATASET_ROOT.resolve()}\n",
    "train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "\n",
    "nc: {len(CLASS_NAMES)}\n",
    "names:\n",
    "  {_names_yaml}\n",
    "\"\"\"\n",
    "\n",
    "if regenerate_yaml.value:\n",
    "    with open(YAML_CONFIG_PATH, \"w\") as _f:\n",
    "        _f.write(_yaml_content)\n",
    "    print(\"✓ Generated data.yaml:\")\n",
    "    print(_yaml_content)\n",
    "else:\n",
    "    print(\"✓ Skipped YAML generation (using existing file)\")\n",
    "\n",
    "print(\"\\nVerifying paths:\")\n",
    "print(f\"Train exists: {TRAINING_IMAGES_PATH.exists()}\")\n",
    "print(f\"Val exists: {VALIDATION_IMAGES_PATH.exists()}\")\n",
    "print(f\"Test exists: {TEST_IMAGES_PATH.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emfo",
   "metadata": {},
   "source": [
    "## Dataset Structure Analysis\n",
    "\n",
    "Examine the distribution of images and labels across train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hstk",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for _split in [\"train\", \"valid\", \"test\"]:\n",
    "    _img_path = DATASET_ROOT / _split / \"images\"\n",
    "    _label_path = DATASET_ROOT / _split / \"labels\"\n",
    "\n",
    "    _num_images = len(list(_img_path.glob(\"*.jpg\")))\n",
    "    _num_labels = len(list(_label_path.glob(\"*.txt\")))\n",
    "\n",
    "    print(f\"{_split.upper():10s}: {_num_images} images, {_num_labels} labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nWHF",
   "metadata": {},
   "source": [
    "## Label Format Inspection\n",
    "\n",
    "YOLO format uses: `class_id x_center y_center width height` (normalized 0-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"SAMPLE LABEL FILE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_label_files = list(TRAINING_LABELS_PATH.glob(\"*.txt\"))\n",
    "_first_label = _label_files[0]\n",
    "\n",
    "print(f\"\\nLabel file: {_first_label.name}\")\n",
    "print(\"\\nContents (first 10 lines):\")\n",
    "with open(_first_label, \"r\") as _f:\n",
    "    _lines = _f.readlines()[:10]\n",
    "    for _i, _line in enumerate(_lines, 1):\n",
    "        print(f\"  {_i}. {_line.strip()}\")\n",
    "\n",
    "print(f\"\\nTotal objects in this image: {len(_lines)}\")\n",
    "print(\"\\nFormat: class_id x_center y_center width height\")\n",
    "print(\"(All values normalized between 0 and 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHCJ",
   "metadata": {},
   "source": [
    "## Class Distribution Analysis\n",
    "\n",
    "Count objects per class across all dataset splits to identify class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ROlb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_all_class_ids = []\n",
    "\n",
    "for _split in [\"train\", \"valid\", \"test\"]:\n",
    "    _label_path = DATASET_ROOT / _split / \"labels\"\n",
    "\n",
    "    for _label_file in _label_path.glob(\"*.txt\"):\n",
    "        with open(_label_file, \"r\") as _f:\n",
    "            for _line in _f:\n",
    "                _parts = _line.strip().split()\n",
    "                if _parts:\n",
    "                    _class_id = int(_parts[0])\n",
    "                    _all_class_ids.append(_class_id)\n",
    "\n",
    "_class_counts = Counter(_all_class_ids)\n",
    "\n",
    "print(f\"\\nTotal objects across all images: {len(_all_class_ids)}\")\n",
    "print(f\"Number of unique classes: {len(_class_counts)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for _class_id in sorted(_class_counts.keys()):\n",
    "    _count = _class_counts[_class_id]\n",
    "    _percentage = (_count / len(_all_class_ids)) * 100\n",
    "    print(f\"  Class {_class_id}: {_count:5d} objects ({_percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qnkX",
   "metadata": {},
   "source": [
    "# Actual Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TqIu",
   "metadata": {},
   "source": [
    "## Sample Images Visualization\n",
    "\n",
    "Display training images with ground truth bounding boxes overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vxnm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes_on_image(img_path, label_path, class_names, colors):\n",
    "    \"\"\"Draw YOLO bounding boxes on image\"\"\"\n",
    "    _img = cv2.imread(str(img_path))\n",
    "    _img = cv2.cvtColor(_img, cv2.COLOR_BGR2RGB)\n",
    "    _h, _w = _img.shape[:2]\n",
    "\n",
    "    with open(label_path, \"r\") as _f:\n",
    "        for _line in _f:\n",
    "            _parts = _line.strip().split()\n",
    "            if not _parts:\n",
    "                continue\n",
    "\n",
    "            _class_id = int(_parts[0])\n",
    "            _x_center = float(_parts[1])\n",
    "            _y_center = float(_parts[2])\n",
    "            _width = float(_parts[3])\n",
    "            _height = float(_parts[4])\n",
    "\n",
    "            # Convert YOLO format to pixel coordinates\n",
    "            _x1 = int((_x_center - _width / 2) * _w)\n",
    "            _y1 = int((_y_center - _height / 2) * _h)\n",
    "            _x2 = int((_x_center + _width / 2) * _w)\n",
    "            _y2 = int((_y_center + _height / 2) * _h)\n",
    "\n",
    "            # Draw rectangle\n",
    "            _color = colors.get(_class_id, (255, 255, 255))\n",
    "            cv2.rectangle(_img, (_x1, _y1), (_x2, _y2), _color, 2)\n",
    "\n",
    "            # Add label\n",
    "            _label = class_names.get(_class_id, f\"Class {_class_id}\")\n",
    "            _label_size, _baseline = cv2.getTextSize(\n",
    "                _label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n",
    "            )\n",
    "            _y1_label = max(_y1, _label_size[1] + 10)\n",
    "            cv2.rectangle(\n",
    "                _img,\n",
    "                (_x1, _y1_label - _label_size[1] - 10),\n",
    "                (_x1 + _label_size[0], _y1_label + _baseline - 10),\n",
    "                _color,\n",
    "                cv2.FILLED,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                _img,\n",
    "                _label,\n",
    "                (_x1, _y1_label - 7),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (0, 0, 0),\n",
    "                2,\n",
    "            )\n",
    "\n",
    "    return _img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DnEU",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"VISUALIZING SAMPLE IMAGES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_image_files = list(TRAINING_IMAGES_PATH.glob(\"*.jpg\"))[:NUM_SAMPLE_IMAGES]\n",
    "_fig, _axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "_axes = _axes.flatten()\n",
    "\n",
    "for _idx, _img_file in enumerate(_image_files):\n",
    "    _label_file = TRAINING_LABELS_PATH / (_img_file.stem + \".txt\")\n",
    "\n",
    "    if _label_file.exists():\n",
    "        _img_with_boxes = draw_boxes_on_image(\n",
    "            _img_file, _label_file, CLASS_NAMES, BBOX_COLORS\n",
    "        )\n",
    "        _axes[_idx].imshow(_img_with_boxes)\n",
    "        _axes[_idx].set_title(f\"Image: {_img_file.name}\", fontsize=10)\n",
    "        _axes[_idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLegend:\")\n",
    "print(\"  Green: Hardhat, Safety Vest (PPE worn correctly)\")\n",
    "print(\"  Red: NO-Hardhat, NO-Mask, NO-Safety Vest (violations)\")\n",
    "print(\"  Magenta: Person\")\n",
    "print(\"  Orange: Safety Cone\")\n",
    "print(\"  Gray: Machinery\")\n",
    "print(\"  Blue: Vehicle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ulZA",
   "metadata": {},
   "source": [
    "## Data Quality Notes\n",
    "\n",
    "Some images have labeling ambiguities: misclassified clothing items or crowded scenes with occlusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfG",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"DATA QUALITY OBSERVATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_cooccurrence = defaultdict(int)\n",
    "\n",
    "for _label_file in TRAINING_LABELS_PATH.glob(\"*.txt\"):\n",
    "    _classes_in_image = set()\n",
    "    with open(_label_file, \"r\") as _f:\n",
    "        for _line in _f:\n",
    "            _parts = _line.strip().split()\n",
    "            if _parts:\n",
    "                _classes_in_image.add(int(_parts[0]))\n",
    "\n",
    "    # Check for suspicious combinations (Person with contradictory PPE states)\n",
    "    if 5 in _classes_in_image:\n",
    "        if 7 in _classes_in_image and 4 in _classes_in_image:\n",
    "            _cooccurrence[\"Person with BOTH vest AND no-vest\"] += 1\n",
    "        if 0 in _classes_in_image and 2 in _classes_in_image:\n",
    "            _cooccurrence[\"Person with BOTH hardhat AND no-hardhat\"] += 1\n",
    "\n",
    "print(\"\\nPotential labeling inconsistencies found:\")\n",
    "for _issue, _count in _cooccurrence.items():\n",
    "    print(f\"  {_issue}: {_count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pvdt",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Load pre-trained YOLOv8 model and fine-tune on PPE detection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBYS",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 nano model pre-trained on COCO dataset (80 classes)\n",
    "pretrained_model = YOLO(PRETRAINED_MODEL_PATH)\n",
    "pretrained_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aLJB",
   "metadata": {},
   "source": [
    "## Baseline Performance\n",
    "\n",
    "Test pre-trained COCO model on construction site images to establish baseline detection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nHfw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Use TEST set for unbiased baseline evaluation\n",
    "pretrained_test_images = list(TEST_IMAGES_PATH.glob(\"*.jpg\"))[:NUM_BASELINE_TEST_SAMPLES]\n",
    "\n",
    "print(\"Testing pre-trained COCO model on TEST set (unbiased baseline):\")\n",
    "for _img_file in pretrained_test_images:\n",
    "    _results = pretrained_model.predict(\n",
    "        source=str(_img_file),\n",
    "        conf=CONFIDENCE_THRESHOLD,\n",
    "        save=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    _result = _results[0]\n",
    "    print(f\"{_img_file.name}: {len(_result.boxes)} objects detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xXTn",
   "metadata": {},
   "outputs": [],
   "source": [
    "_fig, _axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for _idx, _img_file in enumerate(pretrained_test_images):\n",
    "    _results = pretrained_model.predict(\n",
    "        source=str(_img_file), conf=CONFIDENCE_THRESHOLD, save=False, verbose=False\n",
    "    )\n",
    "\n",
    "    _result = _results[0]\n",
    "    _plotted_img = _result.plot()\n",
    "    _plotted_img_rgb = cv2.cvtColor(_plotted_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    _axes[_idx].imshow(_plotted_img_rgb)\n",
    "    _axes[_idx].set_title(f\"{_img_file.name}\\n{len(_result.boxes)} detections\")\n",
    "    _axes[_idx].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AjVT",
   "metadata": {},
   "source": [
    "<span id=\"training-configuration\"></span>\n",
    "\n",
    "## Training Configuration\n",
    "\n",
    "Training progress will be displayed below. Key metrics to monitor:\n",
    "- **mAP50**: Mean Average Precision at IoU=0.5 (higher is better, target >0.7)\n",
    "- **Loss**: Should decrease steadily over epochs\n",
    "- **Precision/Recall**: Balance between false positives and false negatives\n",
    "\n",
    "**Estimated time**: 30-60 minutes on GPU, 4-8 hours on CPU (50 epochs)\n",
    "\n",
    "Click the button below when ready to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pHFh",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_button = mo.ui.run_button(label=\"Train Model\")\n",
    "train_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NCOB",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(not train_button.value, mo.md(\"Press 'Train Model' button to start training\"))\n",
    "\n",
    "print(f\"Training with parameters:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Image Size: {IMAGE_SIZE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "\n",
    "training_results = pretrained_model.train(\n",
    "    data=str(YAML_CONFIG_PATH),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMAGE_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    device=DEVICE,\n",
    "    project=TRAINING_OUTPUT_DIR,\n",
    "    name=TRAINING_RUN_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqbW",
   "metadata": {},
   "source": [
    "## Training Results Analysis\n",
    "\n",
    "Load trained model and visualize training metrics, confusion matrix, and validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TRpd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(\n",
    "    not TRAINED_MODEL_PATH.exists(),\n",
    "    mo.md(f\"**Trained model not found at `{TRAINED_MODEL_PATH}`. Please train the model first.**\"),\n",
    ")\n",
    "\n",
    "trained_model = YOLO(TRAINED_MODEL_PATH)\n",
    "print(f\"Loaded trained model from: {TRAINED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TXez",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(\n",
    "    not TRAINED_MODEL_PATH.exists(),\n",
    "    mo.md(\"**Train the model first to see results.**\"),\n",
    ")\n",
    "\n",
    "_results_dir = TRAINED_MODEL_PATH.parent.parent\n",
    "\n",
    "_fig, _axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "_images_to_show = [\n",
    "    (\"results.png\", \"Training Metrics\"),\n",
    "    (\"confusion_matrix_normalized.png\", \"Confusion Matrix\"),\n",
    "    (\"val_batch0_labels.jpg\", \"Validation: Ground Truth\"),\n",
    "    (\"val_batch0_pred.jpg\", \"Validation: Predictions\"),\n",
    "]\n",
    "\n",
    "for _idx, (_img_name, _title) in enumerate(_images_to_show):\n",
    "    _img_path = _results_dir / _img_name\n",
    "    if _img_path.exists():\n",
    "        _img = mpimg.imread(str(_img_path))\n",
    "        _axes[_idx // 2, _idx % 2].imshow(_img)\n",
    "        _axes[_idx // 2, _idx % 2].set_title(_title)\n",
    "        _axes[_idx // 2, _idx % 2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dNNg",
   "metadata": {},
   "source": [
    "## Detailed Training Metrics\n",
    "\n",
    "Large-format visualizations for detailed analysis of training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCnT",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(\n",
    "    not TRAINED_MODEL_PATH.exists(),\n",
    "    mo.md(\"**Train the model first to see detailed results.**\"),\n",
    ")\n",
    "\n",
    "_results_dir = TRAINED_MODEL_PATH.parent.parent\n",
    "\n",
    "# Training Metrics\n",
    "_fig1 = plt.figure(figsize=(20, 12))\n",
    "_img1 = mpimg.imread(str(_results_dir / \"results.png\"))\n",
    "plt.imshow(_img1)\n",
    "plt.title(\"Training Metrics (Loss, Precision, Recall, mAP)\", fontsize=16)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "_fig2 = plt.figure(figsize=(16, 16))\n",
    "_img2 = mpimg.imread(str(_results_dir / \"confusion_matrix_normalized.png\"))\n",
    "plt.imshow(_img2)\n",
    "plt.title(\"Confusion Matrix (Normalized)\", fontsize=16)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Validation Comparison\n",
    "_fig3, _axes3 = plt.subplots(1, 2, figsize=(24, 12))\n",
    "_img_labels = mpimg.imread(str(_results_dir / \"val_batch0_labels.jpg\"))\n",
    "_img_preds = mpimg.imread(str(_results_dir / \"val_batch0_pred.jpg\"))\n",
    "\n",
    "_axes3[0].imshow(_img_labels)\n",
    "_axes3[0].set_title(\"Validation: Ground Truth Labels\", fontsize=14)\n",
    "_axes3[0].axis(\"off\")\n",
    "\n",
    "_axes3[1].imshow(_img_preds)\n",
    "_axes3[1].set_title(\"Validation: Model Predictions\", fontsize=14)\n",
    "_axes3[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wlCL",
   "metadata": {},
   "source": [
    "## Interactive Training Results\n",
    "\n",
    "Zoomable Bokeh visualizations of training results with pan and zoom capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kqZH",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(\n",
    "    not TRAINED_MODEL_PATH.exists(),\n",
    "    mo.md(\"**Train the model first to see interactive results.**\"),\n",
    ")\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "_results_dir = TRAINED_MODEL_PATH.parent.parent\n",
    "\n",
    "def _show_image_bokeh(img_path, title, width=1200, height=800):\n",
    "    \"\"\"Display image with Bokeh for interactive exploration\"\"\"\n",
    "    _img = np.array(Image.open(img_path))\n",
    "\n",
    "    # Convert to RGBA\n",
    "    if _img.ndim == 2:\n",
    "        _img_rgba = np.stack(\n",
    "            [_img, _img, _img, np.full(_img.shape, 255, dtype=np.uint8)],\n",
    "            axis=2,\n",
    "        )\n",
    "    elif _img.shape[2] == 3:\n",
    "        _img_rgba = np.dstack([_img, np.full(_img.shape[:2], 255, dtype=np.uint8)])\n",
    "    else:\n",
    "        _img_rgba = _img\n",
    "\n",
    "    _img_rgba = np.flipud(_img_rgba)\n",
    "    _img_uint32 = _img_rgba.view(dtype=np.uint32).reshape(_img_rgba.shape[:2])\n",
    "\n",
    "    _h, _w = _img_uint32.shape\n",
    "\n",
    "    _p = figure(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        title=title,\n",
    "        x_range=(0, _w),\n",
    "        y_range=(0, _h),\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
    "    )\n",
    "\n",
    "    _p.image_rgba(image=[_img_uint32], x=0, y=0, dw=_w, dh=_h)\n",
    "    _p.axis.visible = False\n",
    "\n",
    "    return _p\n",
    "\n",
    "_p1 = _show_image_bokeh(_results_dir / \"results.png\", \"Training Metrics\", 1400, 900)\n",
    "_p2 = _show_image_bokeh(\n",
    "    _results_dir / \"confusion_matrix_normalized.png\", \"Confusion Matrix\", 1000, 1000\n",
    ")\n",
    "_p3 = _show_image_bokeh(_results_dir / \"val_batch0_labels.jpg\", \"Ground Truth\", 1200, 800)\n",
    "_p4 = _show_image_bokeh(_results_dir / \"val_batch0_pred.jpg\", \"Predictions\", 1200, 800)\n",
    "\n",
    "show(column(_p1, _p2, _p3, _p4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAgl",
   "metadata": {},
   "source": [
    "## Model Comparison: Pre-trained vs Fine-tuned\n",
    "\n",
    "Side-by-side comparison of COCO pre-trained model vs PPE fine-tuned model on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rEll",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo.stop(\n",
    "    not TRAINED_MODEL_PATH.exists(),\n",
    "    mo.md(\"**Train the model first to see comparison.**\"),\n",
    ")\n",
    "\n",
    "_comparison_images = list(TEST_IMAGES_PATH.glob(\"*.jpg\"))[:NUM_COMPARISON_IMAGES]\n",
    "\n",
    "_fig, _axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for _idx, _img_file in enumerate(_comparison_images):\n",
    "    # Pre-trained COCO model\n",
    "    _results_pretrained = pretrained_model.predict(\n",
    "        str(_img_file), conf=CONFIDENCE_THRESHOLD, verbose=False\n",
    "    )\n",
    "    _img_pretrained = _results_pretrained[0].plot()\n",
    "    _img_pretrained_rgb = cv2.cvtColor(_img_pretrained, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Fine-tuned PPE model\n",
    "    _results_finetuned = trained_model.predict(\n",
    "        str(_img_file), conf=CONFIDENCE_THRESHOLD, verbose=False\n",
    "    )\n",
    "    _img_finetuned = _results_finetuned[0].plot()\n",
    "    _img_finetuned_rgb = cv2.cvtColor(_img_finetuned, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display\n",
    "    _axes[0, _idx].imshow(_img_pretrained_rgb)\n",
    "    _axes[0, _idx].set_title(\n",
    "        f\"COCO: {len(_results_pretrained[0].boxes)} detections\", fontsize=10\n",
    "    )\n",
    "    _axes[0, _idx].axis(\"off\")\n",
    "\n",
    "    _axes[1, _idx].imshow(_img_finetuned_rgb)\n",
    "    _axes[1, _idx].set_title(\n",
    "        f\"PPE: {len(_results_finetuned[0].boxes)} detections\", fontsize=10\n",
    "    )\n",
    "    _axes[1, _idx].axis(\"off\")\n",
    "\n",
    "_fig.text(\n",
    "    0.02,\n",
    "    0.75,\n",
    "    \"Pre-trained\\n(COCO)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    va=\"center\",\n",
    "    rotation=90,\n",
    ")\n",
    "_fig.text(\n",
    "    0.02,\n",
    "    0.25,\n",
    "    \"Fine-tuned\\n(PPE)\",\n",
    "    fontsize=14,\n",
    "    weight=\"bold\",\n",
    "    va=\"center\",\n",
    "    rotation=90,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dGlV",
   "metadata": {},
   "source": [
    "## Using the Trained Model\n",
    "\n",
    "To use the trained model on new images:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load trained model\n",
    "model = YOLO(\"runs/train/ppe_detection/weights/best.pt\")\n",
    "\n",
    "# Run inference\n",
    "results = model.predict(\n",
    "    source=\"path/to/image.jpg\",\n",
    "    conf=0.25,\n",
    "    save=True,\n",
    "    save_txt=True  # Save labels in YOLO format\n",
    ")\n",
    "\n",
    "# Get detections\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        print(f\"Detected: {CLASS_NAMES[cls]} (confidence: {conf:.2f})\")\n",
    "```\n",
    "\n",
    "The model will save annotated images to `runs/detect/predict/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SdmI",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Q: Training fails with CUDA out of memory**\n",
    "- Reduce BATCH_SIZE in constants cell (try 8, then 4)\n",
    "- Reduce IMAGE_SIZE to 416\n",
    "- Close other GPU-intensive applications\n",
    "\n",
    "**Q: Training is very slow**\n",
    "- Check DEVICE is set to GPU (should see \"GPU detected\" message)\n",
    "- If on CPU, reduce EPOCHS to 10 for faster iteration\n",
    "- Ensure CUDA drivers are properly installed\n",
    "\n",
    "**Q: Poor detection performance (low mAP)**\n",
    "- Check class distribution - severe imbalance may need data augmentation\n",
    "- Increase EPOCHS (try 100)\n",
    "- Try larger YOLO models (yolov8s.pt, yolov8m.pt)\n",
    "- Verify dataset labels are correct\n",
    "\n",
    "**Q: Model doesn't exist error**\n",
    "- Click \"Train Model\" button and wait for training to complete\n",
    "- Check that TRAINING_RUN_NAME matches the actual folder in `runs/train/`\n",
    "- Verify TRAINED_MODEL_PATH points to correct location\n",
    "\n",
    "**Q: Dataset not found error**\n",
    "- Ensure dataset is extracted to `data/archive/css-data/`\n",
    "- Check directory structure matches expected layout\n",
    "- Verify all splits (train/valid/test) exist\n",
    "\n",
    "**Q: Import errors or missing packages**\n",
    "- Run: `uv sync` to install all dependencies\n",
    "- Check that you're using Python 3.13+\n",
    "- Try: `uv add torch` if GPU detection fails"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
